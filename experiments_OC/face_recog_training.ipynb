{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import torchvision\n",
    "import random\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Parameters\n",
    "\n",
    "- 학습 시 Augmentation 용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agumentation setting of training dataset \n",
    "train_seq = iaa.Sequential([\n",
    "    iaa.GaussianBlur(sigma=(0, 0.3)),\n",
    "    iaa.Dropout((0.01, 0.15), per_channel=0.5),\n",
    "    iaa.Affine(\n",
    "#         scale={\"x\": (0.95, 1.05), \"y\": (0.95, 1.05)},\n",
    "        translate_percent={\"x\": (-0.15, 0.2), \"y\": (-0.15, 0.2)},\n",
    "        order=[0, 1],\n",
    "        cval=1\n",
    "    )\n",
    "], random_order=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, meta, degree):\n",
    "        with open(meta, 'r') as fin:\n",
    "            self.x = [x for x in fin]\n",
    "\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.RandomRotation(degree),\n",
    "            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.x[idx].split('|')\n",
    "        \n",
    "        # for jiwon server setting\n",
    "\n",
    "        file = Image.fromarray(cv2.imread(entry[0]))\n",
    "        file = self.transform(file)\n",
    "\n",
    "        n_id = entry[1]\n",
    "\n",
    "        return file, np.array(n_id, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from pretrainedmodels import xception\n",
    "\n",
    "\n",
    "\n",
    "class FaceNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(num_classes)\n",
    "        self.basenet = resnet18(num_classes=num_classes) # feature 차원 갯수 -> 이후 작업 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.basenet(x)\n",
    "        x = torch.nn.functional.normalize(x)\n",
    "        return x\n",
    "    \n",
    "class FaceNetX(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(num_classes)\n",
    "        self.pretrained_model = xception(pretrained='imagenet')\n",
    "        self.fc_input = 2048\n",
    "        self.activation = nn.ReLU()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.)\n",
    "        self.last_linear = nn.Linear(self.fc_input, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model.features(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_linear(x)\n",
    "        x = torch.nn.functional.normalize(x)\n",
    "        return x\n",
    "    \n",
    "class FaceCentroids(torch.nn.Module): # 무게중심 \n",
    "    def __init__(self, n_ids, n_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = torch.nn.Parameter(torch.normal(0, 0.01, (n_ids, n_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.nn.functional.linear(\n",
    "            torch.nn.functional.normalize(x), \n",
    "            torch.nn.functional.normalize(self.weight))\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ArcFace(torch.nn.Module):\n",
    "    def __init__(self, sp=64.0, sn=64.0, m=0.5, **kwargs):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.sp = sp / sn  # sn will be multiplied again\n",
    "        self.sn = sn\n",
    "        self.m = m\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "\n",
    "    def forward(self, cosine: torch.Tensor, label):\n",
    "        cosine = cosine.clamp(-1, 1)  # for numerical stability\n",
    "        index = torch.where(label != -1)[0]\n",
    "       \n",
    "        cos_theta = cosine[index]\n",
    "        target_logit = cos_theta[torch.arange(0, cos_theta.size(0)), label[index]].view(\n",
    "            -1, 1\n",
    "        )\n",
    "        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n",
    "        cos_theta_m = (target_logit * self.cos_m - sin_theta * self.sin_m).to(\n",
    "            cosine.dtype\n",
    "        )  # cos(target+margin)\n",
    "\n",
    "        cosine[index] = cosine[index].scatter(\n",
    "            1, label[index, None], cos_theta_m * self.sp\n",
    "        )\n",
    "        cosine.mul_(self.sn)\n",
    "\n",
    "        return cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainining the model\n",
    "\n",
    "- Pretrained 모델로 전이학습을 수행하기 때문에 아래 모델을 직접 학습하지 않고, Synthetic Data로 미리 학습된 모델을 전이학습으로 학습시킴\n",
    "\n",
    "- 아래의 주석은 모델의 정의를 위해 남겨둔 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      ".......................................................................epoch: 0 | loss: 32.2711\n",
      "Accuracy: 0.163 | 0.353 | 0.584 | 0.807 | 0.958\n",
      "====================\n",
      ".......................................................................epoch: 1 | loss: 17.9724\n",
      "Accuracy: 0.430 | 0.769 | 0.947 | 0.994 | 1.000\n",
      "====================\n",
      ".......................................................................epoch: 2 | loss: 3.7085\n",
      "Accuracy: 0.469 | 0.828 | 0.975 | 0.999 | 1.000\n",
      "====================\n",
      ".......................................................................epoch: 3 | loss: 0.7608\n",
      "Accuracy: 0.462 | 0.828 | 0.977 | 0.999 | 1.000\n",
      "====================\n",
      "......................................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jiwon/csiro/deepfake_classification/facialAD/experiments_OC/face_recog_training.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224441534835227d/home/jiwon/csiro/deepfake_classification/facialAD/experiments_OC/face_recog_training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m net \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224441534835227d/home/jiwon/csiro/deepfake_classification/facialAD/experiments_OC/face_recog_training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m loss_accum \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224441534835227d/home/jiwon/csiro/deepfake_classification/facialAD/experiments_OC/face_recog_training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, (img, lbl) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224441534835227d/home/jiwon/csiro/deepfake_classification/facialAD/experiments_OC/face_recog_training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224441534835227d/home/jiwon/csiro/deepfake_classification/facialAD/experiments_OC/face_recog_training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     lbl \u001b[39m=\u001b[39m lbl\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "c = 2\n",
    "\n",
    "# network selecting - ST/Resnet18\n",
    "net = FaceNet(16*c)\n",
    "net = FaceNetX(16*c)\n",
    "\n",
    "net = net.to('cuda:0')\n",
    "\n",
    "fc = FaceCentroids(1002, 16*c)\n",
    "fc = fc.to('cuda:0')\n",
    "\n",
    "loss_arcface = ArcFace(m=0.2)\n",
    "\n",
    "optim = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "batch = 32\n",
    "degree = 20\n",
    "    \n",
    "train_data = 'fvc'\n",
    "\n",
    "if train_data == 'fvc':\n",
    "    trainset = FaceDataset('real_meta.txt', 0)\n",
    "    testset = FaceDataset('real_eval_meta.txt', 0)\n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch, shuffle=True, drop_last=True, num_workers=5)\n",
    "dataloader_val = torch.utils.data.DataLoader(testset, batch_size=batch, shuffle=False, drop_last=False, num_workers=5)\n",
    "\n",
    "for epoch in range(100):\n",
    "    net = net.train()\n",
    "    loss_accum = []\n",
    "\n",
    "    for idx, (img, lbl) in enumerate(dataloader):\n",
    "        img = img.to('cuda:0')\n",
    "        lbl = lbl.to('cuda:0')\n",
    "\n",
    "        feat = net(img)\n",
    "        logit = fc(feat)\n",
    "        # print(logit)\n",
    "        logit = loss_arcface(logit, lbl)\n",
    "        # print(logit)\n",
    "        loss = torch.nn.functional.cross_entropy(logit, lbl)\n",
    "        loss_accum.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        if idx % 4 == 0:\n",
    "            print('.', end='')\n",
    "\n",
    "    loss_accum = torch.tensor(loss_accum)\n",
    "    print(f'epoch: {epoch} | loss: {loss_accum.mean().item():.04f}')\n",
    "\n",
    "    net = net.eval()\n",
    "    \n",
    "    eval_results = []\n",
    "    lbls = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (img, lbl) in enumerate(dataloader_val):\n",
    "            img = img.to('cuda:0')\n",
    "            feat = net(img)\n",
    "         \n",
    "            eval_results.append(feat.to('cpu'))\n",
    "            lbls.append(lbl)\n",
    "           \n",
    "\n",
    "        eval_results = torch.cat(eval_results)\n",
    "        mat_similarity = eval_results.matmul(eval_results.T)\n",
    "\n",
    "        lbls = torch.cat(lbls)\n",
    "        lbls = lbls.view(-1, lbls.size(0)) == lbls.view(lbls.size(0), -1)\n",
    "\n",
    "        accuracy = []\n",
    "\n",
    "        total_comp = torch.ones_like(mat_similarity).triu(1)\n",
    "        total_comp = total_comp.sum().item()\n",
    "\n",
    "        for threshold in [0.0, 0.2, 0.4, 0.6, 0.8]:\n",
    "            threshed = mat_similarity > threshold\n",
    "            \n",
    "            #remove diagonal\n",
    "            correct = (threshed == lbls).triu(1).sum()\n",
    "\n",
    "            accuracy.append(correct / total_comp)\n",
    "            \n",
    "        \n",
    "        print(f'Accuracy: {\" | \".join(f\"{acc:.03f}\" for acc in accuracy)}')\n",
    "\n",
    "    print('=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n",
      "torch.Size([32, 1002])\n",
      "torch.Size([32, 1002])\n"
     ]
    }
   ],
   "source": [
    "for idx, (img, lbl) in enumerate(dataloader):\n",
    "    img = img.to('cuda:0')\n",
    "    lbl = lbl.to('cuda:0')\n",
    "\n",
    "    feat = net(img)\n",
    "    print(feat.shape)\n",
    "    logit = fc(feat)\n",
    "    print(logit.shape)\n",
    "    logit = loss_arcface(logit, lbl)\n",
    "    print(logit.shape)\n",
    "    loss = torch.nn.functional.cross_entropy(logit, lbl)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = os.path.join(\"/media/data2/jiwon/DeepfakeAD\")\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "snapshot = {\n",
    "    \"model_dict\": net.state_dict(),\n",
    "}\n",
    "torch.save(snapshot, os.path.join(result_path, f'recog_model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csiro",
   "language": "python",
   "name": "csiro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
