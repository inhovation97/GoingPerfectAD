
Epoch: 1/50
TRAIN [ 100/2250] Loss: 0.2351 (0.3960) Acc: 82.125% LR: 1.000e-03 Time: 0.205s,  156.39/s (0.313s,  102.14/s) Data: 0.009 (0.105)
TRAIN [ 200/2250] Loss: 0.4847 (0.3197) Acc: 86.094% LR: 1.000e-03 Time: 0.376s,   85.01/s (0.338s,   94.65/s) Data: 0.009 (0.056)
TRAIN [ 300/2250] Loss: 0.1768 (0.2799) Acc: 88.177% LR: 1.000e-03 Time: 0.373s,   85.77/s (0.351s,   91.21/s) Data: 0.004 (0.040)
TRAIN [ 400/2250] Loss: 0.3196 (0.2480) Acc: 89.641% LR: 1.000e-03 Time: 0.388s,   82.49/s (0.357s,   89.52/s) Data: 0.005 (0.032)
TRAIN [ 500/2250] Loss: 0.1627 (0.2270) Acc: 90.631% LR: 1.000e-03 Time: 0.378s,   84.55/s (0.362s,   88.50/s) Data: 0.008 (0.027)
TRAIN [ 600/2250] Loss: 0.4439 (0.2124) Acc: 91.276% LR: 1.000e-03 Time: 0.380s,   84.12/s (0.364s,   87.82/s) Data: 0.006 (0.024)
TRAIN [ 700/2250] Loss: 0.0404 (0.1992) Acc: 91.911% LR: 1.000e-03 Time: 0.393s,   81.39/s (0.367s,   87.30/s) Data: 0.011 (0.022)
TRAIN [ 800/2250] Loss: 0.0647 (0.1882) Acc: 92.355% LR: 1.000e-03 Time: 0.378s,   84.70/s (0.368s,   86.93/s) Data: 0.005 (0.020)
TRAIN [ 900/2250] Loss: 0.1114 (0.1767) Acc: 92.906% LR: 1.000e-03 Time: 0.377s,   84.81/s (0.369s,   86.69/s) Data: 0.006 (0.019)
TRAIN [1000/2250] Loss: 0.0332 (0.1694) Acc: 93.200% LR: 1.000e-03 Time: 0.392s,   81.63/s (0.370s,   86.48/s) Data: 0.019 (0.018)
TRAIN [1100/2250] Loss: 0.1300 (0.1626) Acc: 93.503% LR: 1.000e-03 Time: 0.382s,   83.84/s (0.371s,   86.29/s) Data: 0.006 (0.017)
TRAIN [1200/2250] Loss: 0.0493 (0.1552) Acc: 93.812% LR: 1.000e-03 Time: 0.392s,   81.70/s (0.371s,   86.22/s) Data: 0.017 (0.016)
TRAIN [1300/2250] Loss: 0.1189 (0.1503) Acc: 94.034% LR: 1.000e-03 Time: 0.380s,   84.27/s (0.372s,   86.09/s) Data: 0.009 (0.015)
TRAIN [1400/2250] Loss: 0.1746 (0.1435) Acc: 94.310% LR: 1.000e-03 Time: 0.378s,   84.77/s (0.372s,   85.98/s) Data: 0.006 (0.015)
TRAIN [1500/2250] Loss: 0.0538 (0.1403) Acc: 94.481% LR: 1.000e-03 Time: 0.385s,   83.06/s (0.373s,   85.88/s) Data: 0.008 (0.014)
TRAIN [1600/2250] Loss: 0.1280 (0.1356) Acc: 94.693% LR: 1.000e-03 Time: 0.378s,   84.68/s (0.373s,   85.80/s) Data: 0.006 (0.014)
TRAIN [1700/2250] Loss: 0.0361 (0.1314) Acc: 94.881% LR: 1.000e-03 Time: 0.375s,   85.41/s (0.373s,   85.71/s) Data: 0.004 (0.014)
TRAIN [1800/2250] Loss: 0.1707 (0.1280) Acc: 95.030% LR: 1.000e-03 Time: 0.381s,   84.06/s (0.374s,   85.65/s) Data: 0.008 (0.013)
TRAIN [1900/2250] Loss: 0.0128 (0.1240) Acc: 95.184% LR: 1.000e-03 Time: 0.390s,   82.02/s (0.374s,   85.56/s) Data: 0.006 (0.013)
TRAIN [2000/2250] Loss: 0.0655 (0.1203) Acc: 95.331% LR: 1.000e-03 Time: 0.379s,   84.52/s (0.374s,   85.49/s) Data: 0.006 (0.013)
TRAIN [2100/2250] Loss: 0.0089 (0.1176) Acc: 95.429% LR: 1.000e-03 Time: 0.381s,   84.10/s (0.375s,   85.44/s) Data: 0.007 (0.013)
TRAIN [2200/2250] Loss: 0.0552 (0.1157) Acc: 95.526% LR: 1.000e-03 Time: 0.380s,   84.18/s (0.375s,   85.38/s) Data: 0.007 (0.012)
TEST [101/435]: Loss: 0.114 | Acc: 95.637% [3091/3232]
TEST [201/435]: Loss: 0.134 | Acc: 94.527% [6080/6432]
TEST [301/435]: Loss: 0.121 | Acc: 95.193% [9169/9632]
TEST [401/435]: Loss: 0.121 | Acc: 95.129% [12207/12832]
Best Accuracy 0.000% to 94.381%
Epoch: 2/50
TRAIN [ 100/2250] Loss: 0.3018 (0.0465) Acc: 98.500% LR: 1.000e-03 Time: 0.346s,   92.48/s (0.356s,   89.88/s) Data: 0.007 (0.026)
TRAIN [ 200/2250] Loss: 0.0582 (0.0483) Acc: 98.375% LR: 1.000e-03 Time: 0.371s,   86.30/s (0.352s,   90.80/s) Data: 0.006 (0.017)
TRAIN [ 300/2250] Loss: 0.0043 (0.0522) Acc: 98.250% LR: 1.000e-03 Time: 0.188s,  169.87/s (0.355s,   90.11/s) Data: 0.006 (0.014)
Traceback (most recent call last):
  File "main.py", line 113, in <module>
    run(cfg)
  File "main.py", line 94, in run
    device       = device)
  File "/home/inho/df_detection/experiments_xception/train.py", line 158, in fit
    train_metrics = train(model, trainloader, criterion, optimizer, log_interval, device)
  File "/home/inho/df_detection/experiments_xception/train.py", line 53, in train
    optimizer.step()
  File "/home/inho/anaconda3/envs/df_detection/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/inho/anaconda3/envs/df_detection/lib/python3.7/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/inho/anaconda3/envs/df_detection/lib/python3.7/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/inho/anaconda3/envs/df_detection/lib/python3.7/site-packages/torch/optim/adam.py", line 252, in step
    found_inf=found_inf)
  File "/home/inho/anaconda3/envs/df_detection/lib/python3.7/site-packages/torch/optim/adam.py", line 316, in adam
    found_inf=found_inf)
  File "/home/inho/anaconda3/envs/df_detection/lib/python3.7/site-packages/torch/optim/adam.py", line 364, in _single_tensor_adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
KeyboardInterrupt